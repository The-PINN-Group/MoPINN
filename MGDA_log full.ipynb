{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinntorch import *\n",
    "from functools import partial\n",
    "from matplotlib.transforms import Bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exact_solution_log(x):\n",
    "    return 1/(1+torch.exp(-torch.Tensor(K*x)))\n",
    "\n",
    "# exact solution in NumPy: This one is needed for the loss function becasue somehow the tensor form does not work as of now.\n",
    "def exact_solution_log_np(x):\n",
    "    return 1/(1+np.exp(-K*x))\n",
    "\n",
    "def create_noisy_data(x, std_dev, noise_seed = 42):\n",
    "    exact = exact_solution_log(x)\n",
    "    torch.manual_seed(noise_seed)\n",
    "\n",
    "    return exact + torch.randn(exact.size())*std_dev \n",
    "\n",
    "def data_loss(model: PINN, data: torch.Tensor = None, x: torch.Tensor = None) -> torch.float:\n",
    "    # MSE loss \n",
    "    return (f(model, x) - data).pow(2).mean()\n",
    "\n",
    "def physics_loss(model: PINN, x: torch.Tensor = None) -> torch.float:\n",
    "    # define PDE loss\n",
    "    #x = generate_sample(20, (-1.0, 1.0))\n",
    "    pde_loss_pre = df(model, x) - K*f(model, x)*(1 - f(model, x))\n",
    "    pde_loss = pde_loss_pre.pow(2).mean()\n",
    "    \n",
    "    # define conditional losses (initial + boundary)\n",
    "    boundary_loss_right_pre = (f(model, at(+1.0)) - exact_solution_log_np(+1)) \n",
    "    boundary_loss_right = boundary_loss_right_pre.pow(2).mean()\n",
    "\n",
    "    # combine all losses\n",
    "    final_loss = pde_loss + boundary_loss_right\n",
    "    \n",
    "    return final_loss\n",
    "\n",
    "def total_loss(model: PINN, data: torch.Tensor = None, x: torch.Tensor= None) -> list:\n",
    "\n",
    "    \"\"\"lists both data and physics loss for mgda to work\"\"\"\n",
    "\n",
    "    loss_data = data_loss(model, data, x)\n",
    "\n",
    "    loss_physics = physics_loss(model, x)\n",
    "\n",
    "    return loss_data, loss_physics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def population_training(settings, input_data, train_points, val_points):\n",
    "\n",
    "    L_p = []\n",
    "    L_d = []\n",
    "    L_VAL = []\n",
    "    LR = []\n",
    "    \n",
    "    models_trained = []\n",
    "\n",
    "    torch.manual_seed(settings['model_seed'])\n",
    "    for i in range(settings['population_size']):\n",
    "        print(i)\n",
    "        loss_fn = partial(total_loss,data = input_data, x=train_points)  # For each alpha we need a loss function with different alpha. \n",
    "             \n",
    "        model = PINN(1, 3, 9, 1)\n",
    "        \n",
    "        mgda = WeightMethods(\n",
    "            method=getattr(Moo_method, \"mgda\"),\n",
    "            n_tasks=2,\n",
    "            # normalization=config.moo_normalization, # mgda\n",
    "            device=device,\n",
    "        )\n",
    "\n",
    "        callbacks = [AllDataMonitor(\n",
    "            partial(data_loss, data=input_data, x=train_points), \n",
    "            partial(physics_loss, x=train_points), \n",
    "            partial(physics_loss, x=val_points))]\n",
    "        \n",
    "        trained_model = train_model(\n",
    "            model = model, \n",
    "            loss_fn=loss_fn,\n",
    "            mo_method=mgda,\n",
    "            max_epochs = settings['epochs'],\n",
    "            lr_decay=2e-2,\n",
    "            optimizer_fn = partial(torch.optim.Adam, lr=settings['start_learning_rate']),\n",
    "            epoch_callbacks = callbacks\n",
    "        )\n",
    "\n",
    "        L_p.append(np.array(callbacks[0].physics_history))\n",
    "        L_d.append(np.array(callbacks[0].data_history))\n",
    "        LR.append(np.array(callbacks[0].lr_history))\n",
    "        L_VAL.append(np.array(callbacks[0].val_history))\n",
    "        models_trained.append(trained_model)\n",
    "\n",
    "    return L_p, L_d, LR, L_VAL, models_trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n"
     ]
    }
   ],
   "source": [
    "settings = {}\n",
    "\n",
    "settings['n_data_points'] = 20\n",
    "settings['n_train_points'] = 20\n",
    "settings['n_val_points'] = 39\n",
    "settings['noise_level'] = 0.1\n",
    "settings['epochs'] = 100\n",
    "settings['population_size'] = 50\n",
    "settings['noise_seed'] = 123\n",
    "settings['model_seed'] = 333\n",
    "settings['start_learning_rate'] = 0.01\n",
    "\n",
    "\n",
    "training_points = generate_grid((settings['n_train_points']), (-1.0,1.0))\n",
    "validation_points = generate_grid((settings['n_val_points']), (-1.0,1.0))\n",
    "\n",
    "data_noise = create_noisy_data(training_points, settings['noise_level'], noise_seed=settings['noise_seed'])\n",
    "\n",
    "Loss_physics, Loss_data, LR_evolution, Loss_val, models_trained = population_training(settings, data_noise, training_points, validation_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "settings\n",
      "input_data\n",
      "loss_data\n",
      "loss_physics\n",
      "LR\n",
      "loss_val\n"
     ]
    }
   ],
   "source": [
    "run_name = 'mgda_test'\n",
    "\n",
    "result_dict = {\n",
    "    \"settings\" : settings,\n",
    "    \"input_data\": data_noise.detach().cpu().numpy(),\n",
    "    \"loss_data\": Loss_data,\n",
    "    \"loss_physics\": Loss_physics,\n",
    "    \"LR\": LR_evolution,\n",
    "    \"loss_val\": Loss_val\n",
    "}\n",
    "    \n",
    "path = create_run_folder(run_name)\n",
    "save_dictionary(path, run_name, result_dict)\n",
    "save_models(path, models_trained)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_pareto(points):\n",
    "    \"\"\"\n",
    "    Identify Pareto front indices in a set of points.\n",
    "    :param points: An array of points.\n",
    "    :return: Indices of points in the Pareto front.\n",
    "    \"\"\"\n",
    "    population_size = points.shape[0]\n",
    "    pareto_front = np.ones(population_size, dtype=bool)\n",
    "    for i in range(population_size):\n",
    "        for j in range(population_size):\n",
    "            if all(points[j] <= points[i]) and any(points[j] < points[i]):\n",
    "                pareto_front[i] = 0\n",
    "                break\n",
    "    return pareto_front\n",
    "\n",
    "def plot_pareto_front(\n",
    "    L_D,\n",
    "    L_P,\n",
    "    pareto_indices,\n",
    "    xtick_rotation=0,\n",
    "    file_name=\"pareto_plot\",\n",
    "    bbox_bounds=(0.1, -0.1, 5.2, 3.8),\n",
    "    left_margin=0.15,\n",
    "    bottom_margin=0.15,\n",
    "    x_lim=None,\n",
    "    y_lim=None,\n",
    "):\n",
    "    plt.figure(figsize=(5, 4))\n",
    "\n",
    "    # Plot all points\n",
    "    plt.scatter(L_D, L_P, color=\"gray\", label=\"Dominated\")  # Non-Pareto points in gray\n",
    "    # Plot Pareto points in a different color\n",
    "    plt.scatter(\n",
    "        np.array(L_D)[pareto_indices],\n",
    "        np.array(L_P)[pareto_indices],\n",
    "        color=\"blue\",\n",
    "        label=\"Non-Dominated\",\n",
    "    )  # Pareto points in blue\n",
    "\n",
    "    plt.ylabel(r\"$\\mathcal{L}_\\mathrm{PHYSICS}$\", loc=\"center\", fontsize=13)\n",
    "    plt.xlabel(r\"$\\mathcal{L}_\\mathrm{DATA}$\", loc=\"center\", fontsize=13)\n",
    "    plt.grid()\n",
    "\n",
    "    # Set axis to use scientific notation\n",
    "    plt.ticklabel_format(style=\"sci\", axis=\"both\", scilimits=(-2, 2))\n",
    "    plt.gca().xaxis.get_major_formatter().set_powerlimits((0, 1))\n",
    "    plt.gca().yaxis.get_major_formatter().set_powerlimits((0, 1))\n",
    "\n",
    "    plt.legend()\n",
    "\n",
    "    # Optionally set x and y limits to zoom in\n",
    "    if x_lim is not None:\n",
    "        plt.xlim(x_lim)\n",
    "    if y_lim is not None:\n",
    "        plt.ylim(y_lim)\n",
    "\n",
    "    plt.subplots_adjust(left=left_margin, bottom=bottom_margin)\n",
    "\n",
    "    bbox_instance = Bbox.from_bounds(*bbox_bounds)\n",
    "    plt.xticks(rotation=xtick_rotation)\n",
    "\n",
    "    plt.savefig(\"plots/\" + file_name + \".png\", dpi=600)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pinntest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
