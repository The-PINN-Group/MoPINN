{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinntorch import *\n",
    "from functools import partial\n",
    "from pinntorch.LBFGS import FullBatchLBFGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exact_solution_log(x):\n",
    "    return 1/(1+torch.exp(-torch.Tensor(K*x)))\n",
    "\n",
    "# exact solution in NumPy: This one is needed for the loss function becasue somehow the tensor form does not work as of now.\n",
    "def exact_solution_log_np(x):\n",
    "    return 1/(1+np.exp(-K*x))\n",
    "\n",
    "def create_noisy_data(x, std_dev, noise_seed = 42):\n",
    "    exact = exact_solution_log(x)\n",
    "    torch.manual_seed(noise_seed)\n",
    "\n",
    "    return exact + torch.randn(exact.size())*std_dev \n",
    "\n",
    "def data_loss(model: PINN, data: torch.Tensor = None, x: torch.Tensor = None) -> torch.float:\n",
    "    # MSE loss \n",
    "    return (f(model, x) - data).pow(2).mean()\n",
    "\n",
    "def physics_loss(model: PINN, x: torch.Tensor = None) -> torch.float:\n",
    "    # define PDE loss\n",
    "    #x = generate_sample(20, (-1.0, 1.0))\n",
    "    pde_loss_pre = df(model, x) - K*f(model, x)*(1 - f(model, x))\n",
    "    pde_loss = pde_loss_pre.pow(2).mean()\n",
    "    \n",
    "    # define conditional losses (initial + boundary)\n",
    "    boundary_loss_right_pre = (f(model, at(+1.0)) - exact_solution_log_np(+1)) \n",
    "    boundary_loss_right = boundary_loss_right_pre.pow(2).mean()\n",
    "\n",
    "    # combine all losses\n",
    "    final_loss = pde_loss + boundary_loss_right\n",
    "    \n",
    "    return final_loss\n",
    "\n",
    "def generate_random_mask(size, num_true=5):\n",
    "    mask = torch.zeros(size, dtype=torch.bool)\n",
    "    indices = torch.randperm(size)[:num_true]\n",
    "    mask[indices] = True\n",
    "    return mask\n",
    "\n",
    "def total_loss(model: PINN, data: torch.Tensor = None, x: torch.Tensor= None) -> list:\n",
    "\n",
    "    \"\"\"adds the physics and the data loss with coefficients alpha and (1-alpha) respectively\"\"\"\n",
    "    #mask = generate_random_mask(20, 5)\n",
    "    #masked_data = data[mask]\n",
    "    #masked_x = x[mask]\n",
    "\n",
    "    loss_data = data_loss(model, data, x)\n",
    "\n",
    "    loss_physics = physics_loss(model, x)\n",
    "\n",
    "    return loss_data, loss_physics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValLRMonitor(EpochCallBack):\n",
    "    \"\"\"\n",
    "    Abstract base class for epoch callback objects.\n",
    "    \"\"\"\n",
    "    def __init__(self, validation_points):\n",
    "        self.val_points = validation_points\n",
    "        self.val_loss_fn = partial(physics_loss, x=self.val_points)\n",
    "\n",
    "    def prepare(self, max_epochs, model, loss_fn, optimizer):\n",
    "        self.val_history = []\n",
    "        self.lr_history = []\n",
    "\n",
    "    def process(self, epoch, model, loss_fn, optimizer, current_loss, extra_logs):\n",
    "        loss_val = self.val_loss_fn(model)\n",
    "        loss_physics_val = loss_val.detach().numpy()\n",
    "        self.lr_history.append(float(optimizer.param_groups[0][\"lr\"]))\n",
    "        self.val_history.append(loss_physics_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def population_training(settings, input_data, train_points, val_points):\n",
    "\n",
    "    L_p = []\n",
    "    L_d = []\n",
    "    L_VAL = []\n",
    "    LR = []\n",
    "    \n",
    "    models_trained = []\n",
    "\n",
    "    torch.manual_seed(settings['model_seed'])\n",
    "    for i in range(settings['population_size']):\n",
    "        print(i)\n",
    "        loss_fn = partial(total_loss,data = input_data, x=train_points)  # For each alpha we need a loss function with different alpha. \n",
    "             \n",
    "        model = PINN(1, 3, 9, 1)\n",
    "        \n",
    "        mgda = WeightMethods(\n",
    "            method=getattr(Moo_method, \"mgda\"),\n",
    "            n_tasks=2,\n",
    "            # normalization=config.moo_normalization, # mgda\n",
    "            device=device,\n",
    "        )\n",
    "\n",
    "        callbacks = [TrainLossMonitor(), ValLRMonitor(val_points)]\n",
    "        trained_model = train_model(\n",
    "            model = model, \n",
    "            loss_fn=loss_fn,\n",
    "            #mo_method=mgda,\n",
    "            max_epochs = settings['epochs'],\n",
    "            lr_decay=1e-3,\n",
    "            optimizer_fn = partial(torch.optim.SGD, lr=settings['start_learning_rate']),\n",
    "            epoch_callbacks = callbacks\n",
    "        )\n",
    "\n",
    "\n",
    "        L_p.append(np.array(callbacks[0].loss_history[1]))\n",
    "        L_d.append(np.array(callbacks[0].loss_history[0]))\n",
    "        LR.append(np.array(callbacks[1].lr_history))\n",
    "        L_VAL.append(np.array(callbacks[1].val_history))\n",
    "        models_trained.append(trained_model)\n",
    "\n",
    "    return L_p, L_d, LR, L_VAL, models_trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "<generator object Module.parameters at 0x000001993353EB20>\n",
      "[]\n",
      "<generator object Module.parameters at 0x000001993353EB20>\n",
      "[]\n",
      "<generator object Module.parameters at 0x000001993353EB20>\n",
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\Repositories\\MoPINN\\pinntorch\\_training.py:132: RuntimeWarning: No multi-objective optimization method (mo_method) was set, despite defining multiple losses. Training will continue with the sum of the losses!\n",
      "  warnings.warn(\"No multi-objective optimization method (mo_method) was set, despite defining multiple losses. Training will continue with the sum of the losses!\", RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "settings = {}\n",
    "alphas = 1-torch.logspace(start=-2, end=0.0, steps=20, base=80)\n",
    "\n",
    "settings['n_data_points'] = 20\n",
    "settings['n_train_points'] = 20\n",
    "settings['n_val_points'] = 39\n",
    "settings['noise_level'] = 0.1\n",
    "settings['epochs'] = 3\n",
    "settings['population_size'] = 1\n",
    "settings['noise_seed'] = 123\n",
    "settings['model_seed'] = 333\n",
    "\n",
    "training_points = generate_grid((settings['n_train_points']), (-1.0,1.0))\n",
    "validation_points = generate_grid((settings['n_val_points']), (-1.0,1.0))\n",
    "\n",
    "data_noise = create_noisy_data(training_points, settings['noise_level'], noise_seed=settings['noise_seed'])\n",
    "\n",
    "settings['start_learning_rate'] = 0.003\n",
    "learning_rate = settings['start_learning_rate']\n",
    "epochs = settings['epochs']\n",
    "\n",
    "def custom_color_normalize(value):\n",
    "    return value**80\n",
    "\n",
    "#until = [9342, 5509, 5686, 6044, 6874, 8023, 9543, 8622, 2639, 2799, 3227, 3876, 128, 594, 169, 253, 410, 521, 2051, 49999]\n",
    "\n",
    "Loss_physics, Loss_data, LR_evolution, Loss_val, models_trained = population_training(settings, data_noise, training_points, validation_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "settings\n",
      "input_data\n",
      "loss_data\n",
      "loss_physics\n",
      "LR\n",
      "loss_val\n"
     ]
    }
   ],
   "source": [
    "run_name = 'mgda_lbfgs'\n",
    "\n",
    "result_dict = {\n",
    "    \"settings\" : settings,\n",
    "    \"input_data\": data_noise.detach().cpu().numpy(),\n",
    "    \"loss_data\": Loss_data,\n",
    "    \"loss_physics\": Loss_physics,\n",
    "    \"LR\": LR_evolution,\n",
    "    \"loss_val\": Loss_val\n",
    "}\n",
    "    \n",
    "path = create_run_folder(run_name)\n",
    "save_dictionary(path, run_name, result_dict)\n",
    "save_models(path, models_trained)\n",
    "\n",
    "#print(data_noise.shape)\n",
    "#save_results(run_name, settings, data_noise, Loss_data, Loss_physics, LR_evolution, Loss_val, models_trained)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pinntest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
