{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinntorch import *\n",
    "from functools import partial\n",
    "\n",
    "# use GPU for faster training\n",
    "torch.set_default_tensor_type('torch.cuda.FloatTensor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 1.0\n",
    "M = 5.0\n",
    "T = 12.5\n",
    "\n",
    "X_DOMAIN = (0.0, M)\n",
    "T_DOMAIN = (0.0, T)\n",
    "\n",
    "def exact_solution(x, t):\n",
    "    \"\"\"returns the exact solution given the IC and the BC\"\"\"\n",
    "    return torch.sin(torch.pi*x/M)*torch.exp(-k*((torch.pi**2)/(M**2))*t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let us first define some book keeping plotting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(x, t, data, grid_shape):\n",
    "    \"\"\"\n",
    "    Takes the domain points and the result on them to plot a 3D plot.\n",
    "    \"\"\"\n",
    "    z = data\n",
    "    color_map = cm.winter\n",
    "    x = x.cpu().detach().numpy().reshape(grid_shape)\n",
    "    t = t.cpu().detach().numpy().reshape(grid_shape)\n",
    "    z = z.cpu().detach().numpy().reshape(grid_shape)\n",
    "\n",
    "    # Set up plot\n",
    "    fig, ax = plt.subplots(subplot_kw=dict(projection='3d'))\n",
    "    ax.set(xlabel='x (location)', ylabel='t (time)', zlabel='function value')\n",
    "\n",
    "    ls = LightSource(270, 45)\n",
    "\n",
    "    surf = ax.plot_surface(x, t, z, cmap=color_map, linewidth=0, antialiased=False, shade=False)\n",
    "    fig.colorbar(surf, shrink=0.5, aspect=5)\n",
    "\n",
    "def plot_heatmap(x, t, data, grid_shape):\n",
    "    \"\"\"\n",
    "    Takes the domain points and the result on them to plot a heat-map.\n",
    "    \"\"\"\n",
    "    z = data\n",
    "    color_map = plt.cm.winter\n",
    "    x = x.cpu().detach().numpy().reshape(grid_shape)\n",
    "    t = t.cpu().detach().numpy().reshape(grid_shape)\n",
    "    z = z.cpu().detach().numpy().reshape(grid_shape)\n",
    "    print(z)\n",
    "    print((x.min(), x.max(), t.min(), t.max()))\n",
    "\n",
    "    plt.imshow(z, cmap=color_map, aspect='auto', origin='upper', extent=(t.min(), t.max(), x.min(), x.max()))\n",
    "    plt.colorbar(label='Function Value')\n",
    "    plt.xlabel('t (time)')\n",
    "    plt.ylabel('x (location)')\n",
    "    plt.title('Heat Map of Function')\n",
    "    plt.show()\n",
    "\n",
    "def plot_pareto_front(L_D, L_P, data_color, cmap = 'virdis'):\n",
    "    \n",
    "    plt.scatter(L_D, L_P, c=data_color.detach().numpy(), cmap='viridis')  # Use 'viridis' colormap, but you can choose any other\n",
    "    # Add colorbar for the z values\n",
    "    cbar = plt.colorbar()\n",
    "    cbar.set_label(r' $α$ Values')\n",
    "\n",
    "    plt.xlabel(r\"data loss ($L_d$)\")\n",
    "    plt.ylabel(r\"physics loss ($L_p$)\")\n",
    "    plt.title(\"Multi-objective optimization L = $α.L_d + (1-α)L_p$\")\n",
    "\n",
    "    #plt.savefig(\"heat_pareto_unstructured.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def data_loss(model: nn.Module, data: torch.Tensor = None, x: torch.Tensor = None, t:torch.Tensor = None) -> torch.float:\n",
    "    \"\"\"\"Caculates the data loss\"\"\"\n",
    "    u_n = f(model, x, t) # evaluating the model\n",
    "    # MSE loss \n",
    "    diff = u_n - data    # u_exact + gaussian noise \n",
    "    \n",
    "    loss = diff.pow(2).mean()\n",
    "\n",
    "    return loss\n",
    "\n",
    "def physics_loss(\n",
    "    model: nn.Module, x: torch.Tensor = None, t: torch.Tensor = None\n",
    ") -> torch.float:\n",
    "\n",
    "    pde_loss_pre = df(model, x, t, wrt=1, order=1) - k*df(model, x, t, wrt=0, order=2)\n",
    "    pde_loss = pde_loss_pre.pow(2).mean()\n",
    "    \n",
    "\n",
    "    t_raw = unique_excluding(t, 0.)\n",
    "    x_left_boundary = fill_like(t_raw, 0.)\n",
    "    x_right_boundary = fill_like(t_raw, M)\n",
    "    x_raw = unique_excluding(x)\n",
    "    t_zero = fill_like(x_raw, 0.)\n",
    "\n",
    "    # dirichlet boundary conditions.  \n",
    "    boundary_left = f(model, x_left_boundary, t_raw).pow(2).mean()\n",
    "    boundary_right = f(model, x_right_boundary, t_raw).pow(2).mean()\n",
    "    boundary_loss = boundary_left + boundary_right\n",
    "\n",
    "    # initial\n",
    "    initial_loss_pre = f(model, x_raw, t_zero) - torch.sin(np.pi/M * x_raw).reshape(-1, 1)\n",
    "    initial_loss = initial_loss_pre.pow(2).mean()\n",
    "    \n",
    "    # together\n",
    "    conditional_loss = boundary_loss + initial_loss\n",
    "    \n",
    "    final_loss = pde_loss + conditional_loss\n",
    "    return final_loss\n",
    "\n",
    "def val_loss(\n",
    "    model: nn.Module, x: torch.Tensor, t: torch.Tensor) -> torch.float:\n",
    "\n",
    "    pde_loss_pre = df(model, x, t, wrt=1, order=1) - k*df(model, x, t, wrt=0, order=2)\n",
    "    pde_loss = pde_loss_pre.pow(2).mean()\n",
    "    \n",
    "    return pde_loss\n",
    "\n",
    "def total_loss(model: nn.Module, data: torch.Tensor, x_data: torch.Tensor, t_data: torch.Tensor, x_physics: torch.Tensor, t_physics: torch.Tensor, alpha: torch.float) -> torch.float:\n",
    "\n",
    "    loss_data = data_loss(model, data, x_data, t_data)\n",
    "\n",
    "    loss_physics = physics_loss(model, x_physics, t_physics)\n",
    "\n",
    "    total = alpha*loss_data + (1 - alpha)* loss_physics \n",
    "    \n",
    "    return total  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_noisy_data(std_dev, exact_soln):\n",
    "\n",
    "    \"\"\"adds gaussian noise to the data\"\"\"\n",
    "\n",
    "    return exact_soln + torch.randn(exact_soln.shape)*std_dev "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let us plot the exact solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_d, t_train_d = generate_grid((20,20), domain=(X_DOMAIN, T_DOMAIN))\n",
    "x_train_p, t_train_p = generate_grid((20,20), domain=(X_DOMAIN, T_DOMAIN))\n",
    "x_val, t_val = generate_grid((39,39), domain=(X_DOMAIN, T_DOMAIN))\n",
    "x_plot, t_plot = generate_grid((200,200), domain=(X_DOMAIN, T_DOMAIN))\n",
    "\n",
    "exact_soln = exact_solution(x_train_d, t_train_d)\n",
    "\n",
    "plot_solution = exact_solution(x_plot, t_plot)\n",
    "plot_heatmap(x_plot, t_plot, plot_solution, grid_shape=(200, 200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = {}\n",
    "settings['seed'] = 11373\n",
    "torch.manual_seed(settings['seed'])\n",
    "settings['n_train_points'] = 20\n",
    "settings['n_val_points'] = 80\n",
    "\n",
    "settings['noise_level'] = 0.1\n",
    "\n",
    "input_data = create_noisy_data(settings['noise_level'], exact_soln)\n",
    "\n",
    "plot_data(x_train_d, t_train_d, input_data, (20, 20))\n",
    "\n",
    "settings['start_learning_rate'] = 0.003\n",
    "learning_rate = settings['start_learning_rate']\n",
    "epochs = 20_000\n",
    "\n",
    "def custom_color_normalize(value):\n",
    "    return value**80\n",
    "\n",
    "alphas = 1-torch.logspace(start=-2, end=0.0, steps=20, base=80)\n",
    "\n",
    "settings['alphas'] = alphas.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scalar_training(alphas, input_data, x_train_d, t_train_d, x_train_p, t_train_p, x_val, t_val):\n",
    "    L_p = []\n",
    "    L_d = []\n",
    "    L_VAL = []\n",
    "    LR = []\n",
    "    \n",
    "    models_trained = []\n",
    "    for i, alpha in enumerate(alphas):\n",
    "        print(\"i:\", i, \"alpha:\", alpha)\n",
    "        \n",
    "        loss_fn = partial(total_loss,data = input_data, x_data=x_train_d, t_data=t_train_d, x_physics=x_train_p, t_physics=t_train_p, alpha = alpha)\n",
    "        \n",
    "        torch.manual_seed(7245)\n",
    "        model = PINN(2, 4, 50, 1, activation=\"fourier\")\n",
    "        \n",
    " #   def train_model(\n",
    " #   model: nn.Module,\n",
    " #   loss_fn: Callable,\n",
    " #   mo_method: Callable = None,\n",
    " #   optimizer_fn=torch.optim.Adam,\n",
    " #   max_epochs: int = 1_000,\n",
    " #   live_logging: bool = True,\n",
    " #   log_interval: int = 1_000,\n",
    " #   lr_decay = 0.0,\n",
    " #   parameter_groups: dict = None,\n",
    " #   epoch_callbacks: list = [],\n",
    " \n",
    " \n",
    "# callbacks = [ValLRMonitor(training_points=train_points, validation_points=val_points, data_values=input_data)]\n",
    "#        trained_model = train_model(\n",
    "#            model = model, \n",
    "#            loss_fn=loss_fn,\n",
    "#            max_epochs = settings['epochs'],\n",
    "#            lr_decay=2e-2,\n",
    "#            optimizer_fn = partial(torch.optim.Adam, lr=settings['start_learning_rate']),\n",
    "#            epoch_callbacks = callbacks\n",
    "#        )\n",
    "    loss_data = data_loss(model, data, x_data, t_data)\n",
    "    data_loss(model: nn.Module, data: torch.Tensor = None, x: torch.Tensor = None, t:torch.Tensor = None)\n",
    "\n",
    "    loss_physics = physics_loss(model, x_physics, t_physics)\n",
    "        callbacks = [AllDataMonitor(\n",
    "            partial(data_loss, data=input_data, x=x_train_d, t=t_train_d), \n",
    "            partial(physics_loss, x=train_points), \n",
    "            partial(val_loss, x=x_val, t=t_val))]\n",
    "        \n",
    "        trained_model = train_model(\n",
    "        model = model, \n",
    "        loss_fn=loss_fn,\n",
    "        optimizer_fn=partial(torch.optim.Adam, lr=settings['start_learning_rate']),\n",
    "        max_epochs = settings['epochs'],\n",
    "        lr_decay = 2e-2,\n",
    "        epoch_callbacks = callbacks)\n",
    "\n",
    "\n",
    "        L_p.append(np.array(l_p))\n",
    "        L_d.append(np.array(l_d))\n",
    "        LR.append(np.array(lr))\n",
    "        L_VAL.append(np.array(l_val))\n",
    "        models_trained.append(trained_model)\n",
    "\n",
    "    return L_p, L_d, LR, L_VAL, models_trained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with exact solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Loss_p, Loss_d, LR, Loss_VAL, models_trained = scalar_training(alphas, input_data, x_train_d, t_train_d, x_train_p, t_train_p, x_val, t_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name = 'heat_L1_k25_test'\n",
    "\n",
    "result_dict = {\n",
    "    \"settings\" : settings,\n",
    "    \"input_data\": input_data.detach().cpu().numpy(),\n",
    "    \"loss_data\": Loss_d,\n",
    "    \"loss_physics\": Loss_p,\n",
    "    \"LR\": LR,\n",
    "    \"loss_val\": Loss_VAL\n",
    "}\n",
    "\n",
    "path = create_run_folder(run_name)\n",
    "save_dictionary(path, run_name, result_dict)\n",
    "save_models(path, models_trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = \"heat_equation/trained_models\"  \n",
    "os.makedirs(save_dir, exist_ok=True) # TODO: Add a conditional statement to make sure that we are not overwriting the directory. \n",
    "def save_model(model, filename):\n",
    "    filepath = os.path.join(save_dir, filename)\n",
    "    torch.save(model, filepath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#callbacks = [TrainLossMonitor(), TrueErrorMonitor(test_points, logistic_fn), SolutionMonitor(plot_points, training_points, store_every=20)]\n",
    "def weighted_training(alphas, input_data):\n",
    "\n",
    "    #TODO: output error_true_evolution \n",
    "    #TODO: \n",
    "\n",
    "    L_p = torch.zeros_like(alphas) #tensor to save physics loss for each \\alpha \n",
    "    L_d = torch.zeros_like(alphas) #tensor to save data loss for each \\alpha\n",
    "    \n",
    "    loss_train_evolution = np.zeros((len(alphas), epochs))  #tensor to save training evolution for each \\alpha \n",
    "    #error_true_evolution = np.zeros((len(alphas), epochs))  #tensor to save error evolution for each \\alpha\n",
    "\n",
    "    for i, alpha in enumerate(alphas):\n",
    "        \n",
    "        loss_fn = partial(total_loss, data = input_data, x=x_train, t = t_train,  alpha = alpha)  # For each alpha we need a loss function with different alpha. \n",
    "        callbacks = [TrainLossMonitor()]  # TODO: Include True Error Monitor here (for now it is only written for one dimension but here we have two dimensions)\n",
    "        model = PINN(2, 4, 6, 1)\n",
    "        trained_model = train_model(\n",
    "        model = model, \n",
    "        loss_fn = loss_fn, \n",
    "        learning_rate = learning_rate, \n",
    "        max_epochs = epochs, \n",
    "        optimizer_fn = torch.optim.Adam,\n",
    "        epoch_callbacks = callbacks)\n",
    "\n",
    "        train_loss_evolution = callbacks[0].train_loss_history\n",
    "        #true_error_evolution = callbacks[1].mae_history\n",
    "\n",
    "        loss_train_evolution[i, :] = train_loss_evolution\n",
    "        #error_true_evolution[i, :] = true_error_evolution\n",
    "\n",
    "        #calculating the physics losses and data losses from trained model with a loss function dependent on α\n",
    "        l_p = physics_loss(trained_model, x_train, t_train)\n",
    "        l_d = data_loss(trained_model, input_data, x_train, t_train)\n",
    "\n",
    "\n",
    "        L_p[i] = l_p\n",
    "        L_d[i] = l_d\n",
    "\n",
    "        model_filename = f\"model_heat{i}.pth\"  #TODO: Saving the ith model does not give info about α. We need to save by α. So far, it is causing trouble.\n",
    "        save_model(trained_model.state_dict(), model_filename)\n",
    "\n",
    "    return L_p, L_d, loss_train_evolution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas1, alphas2 = torch.linspace(0, 0.95, 20), torch.linspace(0.95, 1, 50)\n",
    "alphas_cat3 = torch.cat((alphas1, alphas2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_physics_cat4, loss_data_cat4, loss_training_evolution_cat4 = weighted_training(alphas_cat3, data.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the saved models. \n",
    "By now, we have saved the models. Now is the time to load them again. First we need to initialize the model and then we can load the models that we  have saved. After loading, we want to try the following. \n",
    "* Plot the Eucledian norms of the models correcponding to each α. \n",
    "* Maybe we can do BGD starting from a particular α and see if we can minimize the loss further. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#current_dir = os.getcwd()\n",
    "#print(current_dir+\"/heat_equation/trained_models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_norm(parameters1, parameters2):\n",
    "    squared_diff_sum = 0.0\n",
    "    for name in parameters1:\n",
    "        diff = parameters1[name] - parameters2[name]\n",
    "        squared_diff_sum += torch.sum(diff ** 2)\n",
    "    return torch.sqrt(squared_diff_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = \"heat_equation/trained_models\"  # directory containing trained models  \n",
    "\n",
    "norm_list = []\n",
    "for i in range(len(alphas_cat3)-1):\n",
    "    model = PINN(2, 4, 6, 1)\n",
    "    model_next = PINN(2, 4, 6, 1)\n",
    "    \n",
    "    model.load_state_dict(torch.load(os.getcwd()+\"/\"+save_dir+f\"/model_heat{i}.pth\"))\n",
    "    model_next.load_state_dict(torch.load(os.getcwd()+\"/\"+save_dir+f\"/model_heat{i+1}.pth\"))\n",
    "\n",
    "    w = model.state_dict()\n",
    "    w_next = model_next.state_dict()\n",
    "\n",
    "    norm_list.append(euclidean_norm(w, w_next))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(norm_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(alphas_cat3[:-1], norm_list)\n",
    "plt.xlabel(r\"$\\alpha \\in [0, 1]$\")\n",
    "plt.ylabel(r\"$||w_i - w_{i+1}||_2$\")\n",
    "plt.title(r\"$||w_i - w_{i+1}||_2$ Vs α\")\n",
    "plt.savefig(\"heat_norms.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(alphas_cat3[30:-1], norm_list[30:])\n",
    "plt.xlabel(r\"$\\alpha \\in [0, 1]$\")\n",
    "plt.ylabel(r\"$||w_i - w_{i+1}||_2$\")\n",
    "plt.title(r\"$||w_i - w_{i+1}||_2$ Vs α\")\n",
    "plt.savefig(\"heat_norms1.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pinngpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
